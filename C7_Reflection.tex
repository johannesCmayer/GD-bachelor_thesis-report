% !TEX root = BachelorBookletMain.tex

\chapter{Reflection}
\section{Project}
The original idea of integrating a GQN into an interactive experience was only partly successful. The time needed to get accustomed with the intricacies of the methods required to use the GQN to its full potential was greater that the time available for research in this project. Only a simplified GQN architecture could be utilized in an interactive prototype.

For the prototypes that where developed, it might be argued that simpler methods exist that could have been used to achieve the same design goal. In the maze game a post processing filter might be applied to make the environment more confounding instead of using a neural network. This is a valid critique. If one wants to create a game similar to the maze game and wants do so quickly one should not use a neural network. However, it is probably the case that the specific style that was achieved in this work, using neural networks, is very hard or practically impossible to recreate using simpler methods.

This work also represents only a first exploration of the possibility space of how neural networks might be used to process visual information before it is presented to the player and the improvements described in \cref{FurtherWork} would make it possible to create even more unique experiences that utilize the power of the GQN to its full potential.

The final version of the maze game prototype, can stand on its own as a playable, challenging game. Only one level is available in the final quality but with the systems in place, adding more levels would be easy. The goal of the maze game prototype of creating a visually simple but appealing game, where the player has to use sound to navigate effectively through a maze, avoid enemies and collect checkpoints, all while deciphering the output of the network has been achieved.

The project greatly improved my understanding about how neural networks function at the core. The model in the project uses synthetic data. This enabled me to learn how to construct a complete pipeline for an unsupervised machine learning system. I implemented data generated and preprocessing, created a simplified version of the GQN architecture and improved my intuitions about how to train neural networks.


\section{Workflow Evaluation}
When working with neural networks it is often necessary to try out different architectures and parameters for a model that should perform a specific task. For this it is very useful to have programmatic utilities, that automatically search the space of parameters for some time and then returns the best. I realized to late into the project how useful these utilities would have been. The technical debt I collected this far into the project was to high and remaining time of the project to short, so that I decided that the implementation would not be worth the effort anymore.

Another mistake I made, was to make the model too soon too complex. Very early on I started to train the model on data of a camera rotating on multiple axis that produces 64x64---instead of 32x32---resolution images as training data. Both these things significantly increased the time necessary to train the network to a reasonable level. This made iteration time slow. Besides the training time issue, the implementation of parameterized capturing and higher resolution output took too soon an unnecessarily large chunk of time out of my schedule, time that would have been better spend improving the existing systems.

Furthermore I sunk some time into researching how to train neural network models on multiple machines. After 1--2 days of research I realized, that the endeavor is outside the scope of this project.

Structuring the project around creating prototypes to evaluate specific ideas worked well. This approach encouraged me to finding out early on all the ways an idea might break. I knew that if an idea did not work out I could move on to developing the next prototype reusing what worked and discarding the rest. This allowed me to have a short iteration cycle when exploring directions for the project. A further advantage was that I started to combine ideas from different prototypes that I would not have thought about without having the prototypes as reference. An example is to reuse the object morphing, that was used in a basic form in the walking simulator prototype, and build a whole prototype just around that idea. This prototype then became then the object morphing prototype.


\section{Other directions}
The GQN is a versatile neural network architecture. There are many directions, different from the ones explored in this work, one might want to go. Some that came to my mind are listed below.

\subsection{GQN and reinforcement learning}
One promising way to apply neural networks to interactive environments is through reinforcement learning. Reinforcement learning allows machine learning agents to learn from experience. They might in the future be used to create more realistic NPCs, generate content such as levels and music, or to open up the world of machine learning to players by letting them train their own agents. The ability of the GQN to create a compact encoding of an environment can be used in conjunction with reinforcement learning.

\cite{gqn} show that it is possible to utilize the representation of a GQN as input to a reinforcement learning algorithm to greatly improve the algorithms data efficiency. This in turn makes an agent learn faster with fewer data points\footnote{The paper reported that $75\%$ less data was required.} making it more practical to use reinforcement learning in interactive environments. In the paper they train an agent to control a robot arm to reach for a randomly positioned sphere. They also show that the agent can learn how to control the arm, even when the camera is positioned at a random position on a sphere around the robot arm each frame.

The invariance to where the camera is positioned could potentially be used to give control of the camera to a player. The player could then, by moving the camera, determine what kind of observations an agent gets from the environment. This is interesting both as a means of giving the player control of the learning process and manipulating inference. During training the player can focus on different objects the agent can interact with. This would enable the player to determine which skills the agent learns by controlling the generation of the training data. During inference the player can influence the already learned behavior by changing the inputs the agent receives by moving the camera e.g. to observe specific objects.

\subsection{Synthesized game}
One might be able to expand the capabilities of the GQN to include sequence modeling. This would mean, that the model is able to not only predict the current environment state but also how this state evolves over time. The model would then be able to predict how a mobile object would move on screen given only images of the environment, the coordinates of where the images where taken, and a reference from which time step each image stems. This might even allow the network to predict game states such as game over and game state changes caused by player input similar to what was shown to be possible by \cite{Ha2018-dd}. This would allow us to train a model on multiple game environments\footnote{By a game environment is meant that the environment also has a specific rule set attached to it.} and then smoothly blend between the different environments, creating a nearly infinite amount of possible gameplay variations.

As \cite{Ha2018-dd} show, this approach might also be combined with reinforcement learning. An agent could be trained on many of the generated gameplay variations. This might result in a more robust agent, which could be useful if the agent uses a virtual environment to train a task that is supposed to be executed in the real world. A related approach to making agents, that train in virtual environments for real world tasks, more robust is discussed by \cite{Peng2017-wx}.


% \subsection{Generating geometry}
% Use GQN to generate 3D objects.
% geometry changes based on player position.
%
%
% \subsection{Code generation}
% Maybe a GQN can be used to generate code. For this we feed in a vector encoded representations of source code. It might be possible to integrate a reinforcement learning agents into this system, that learns to predict if some code would compile based on the representation the GQN learned and would add to the loss accordingly. This might allow us to use gradient decent to minimize the compile errors the output of the model has.
