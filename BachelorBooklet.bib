@ARTICLE{gqn,
  title    = "Neural scene representation and rendering",
  author   = "Eslami, S M Ali and Jimenez Rezende, Danilo and Besse, Frederic
              and Viola, Fabio and Morcos, Ari S and Garnelo, Marta and
              Ruderman, Avraham and Rusu, Andrei A and Danihelka, Ivo and
              Gregor, Karol and Reichert, David P and Buesing, Lars and Weber,
              Theophane and Vinyals, Oriol and Rosenbaum, Dan and Rabinowitz,
              Neil and King, Helen and Hillier, Chloe and Botvinick, Matt and
              Wierstra, Daan and Kavukcuoglu, Koray and Hassabis, Demis",
  abstract = "Scene representation-the process of converting visual sensory
              data into concise descriptions-is a requirement for intelligent
              behavior. Recent work has shown that neural networks excel at
              this task when provided with large, labeled datasets. However,
              removing the reliance on human labeling remains an important open
              problem. To this end, we introduce the Generative Query Network
              (GQN), a framework within which machines learn to represent
              scenes using only their own sensors. The GQN takes as input
              images of a scene taken from different viewpoints, constructs an
              internal representation, and uses this representation to predict
              the appearance of that scene from previously unobserved
              viewpoints. The GQN demonstrates representation learning without
              human labels or domain knowledge, paving the way toward machines
              that autonomously learn to understand the world around them.",
  journal  = "Science",
  volume   =  360,
  number   =  6394,
  pages    = "1204--1210",
  month    =  jun,
  year     =  2018,
  language = "en"
}
