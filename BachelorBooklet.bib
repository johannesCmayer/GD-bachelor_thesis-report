@ARTICLE{gqn,
  title    = "Neural scene representation and rendering",
  author   = "Eslami, S M Ali and Jimenez Rezende, Danilo and Besse, Frederic
              and Viola, Fabio and Morcos, Ari S and Garnelo, Marta and
              Ruderman, Avraham and Rusu, Andrei A and Danihelka, Ivo and
              Gregor, Karol and Reichert, David P and Buesing, Lars and Weber,
              Theophane and Vinyals, Oriol and Rosenbaum, Dan and Rabinowitz,
              Neil and King, Helen and Hillier, Chloe and Botvinick, Matt and
              Wierstra, Daan and Kavukcuoglu, Koray and Hassabis, Demis",
  abstract = "Scene representation-the process of converting visual sensory
              data into concise descriptions-is a requirement for intelligent
              behavior. Recent work has shown that neural networks excel at
              this task when provided with large, labeled datasets. However,
              removing the reliance on human labeling remains an important open
              problem. To this end, we introduce the Generative Query Network
              (GQN), a framework within which machines learn to represent
              scenes using only their own sensors. The GQN takes as input
              images of a scene taken from different viewpoints, constructs an
              internal representation, and uses this representation to predict
              the appearance of that scene from previously unobserved
              viewpoints. The GQN demonstrates representation learning without
              human labels or domain knowledge, paving the way toward machines
              that autonomously learn to understand the world around them.",
  journal  = "Science",
  volume   =  360,
  number   =  6394,
  pages    = "1204--1210",
  month    =  jun,
  year     =  2018,
  language = "en"
}

@ARTICLE{Ioffe2015-eh,
  title         = "Batch Normalization: Accelerating Deep Network Training by
                   Reducing Internal Covariate Shift",
  author        = "Ioffe, Sergey and Szegedy, Christian",
  abstract      = "Training Deep Neural Networks is complicated by the fact
                   that the distribution of each layer's inputs changes during
                   training, as the parameters of the previous layers change.
                   This slows down the training by requiring lower learning
                   rates and careful parameter initialization, and makes it
                   notoriously hard to train models with saturating
                   nonlinearities. We refer to this phenomenon as internal
                   covariate shift, and address the problem by normalizing
                   layer inputs. Our method draws its strength from making
                   normalization a part of the model architecture and
                   performing the normalization for each training mini-batch.
                   Batch Normalization allows us to use much higher learning
                   rates and be less careful about initialization. It also acts
                   as a regularizer, in some cases eliminating the need for
                   Dropout. Applied to a state-of-the-art image classification
                   model, Batch Normalization achieves the same accuracy with
                   14 times fewer training steps, and beats the original model
                   by a significant margin. Using an ensemble of
                   batch-normalized networks, we improve upon the best
                   published result on ImageNet classification: reaching 4.9\%
                   top-5 validation error (and 4.8\% test error), exceeding the
                   accuracy of human raters.",
  month         =  feb,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1502.03167"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Glorot2010-kn,
  title     = "Understanding the difficulty of training deep feedforward neural
               networks",
  author    = "Glorot, X and Bengio, Y",
  abstract  = "Whereas before 2006 it appears that deep multilayer neural
               networks were not successfully trained, since then several
               algorithms have been shown to successfully train them, with
               experimental results showing the superiority of deeper vs less
               deep architectures. All these â€¦",
  journal   = "Proceedings of the thirteenth international conference",
  publisher = "jmlr.org",
  year      =  2010
}

@ARTICLE{Ha2018-dd,
  title         = "World Models",
  author        = "Ha, David and Schmidhuber, J{\"u}rgen",
  abstract      = "We explore building generative neural network models of
                   popular reinforcement learning environments. Our world model
                   can be trained quickly in an unsupervised manner to learn a
                   compressed spatial and temporal representation of the
                   environment. By using features extracted from the world
                   model as inputs to an agent, we can train a very compact and
                   simple policy that can solve the required task. We can even
                   train our agent entirely inside of its own hallucinated
                   dream generated by its world model, and transfer this policy
                   back into the actual environment. An interactive version of
                   this paper is available at https://worldmodels.github.io/",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1803.10122"
}
