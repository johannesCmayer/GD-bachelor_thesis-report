% !TEX root = BachelorBookletMain.tex

\chapter{Neural Network Systems}
\section{Model}
{To create the neural network model used in this project the Kears funtional API is used. Keas is the official frontend to Tensorflow and simplifies the creation of neural networks. With the functional API predefined callable class instances---which represent layers---can be used to define model architectures. For this a class of a layer type is set up and called with the input to the layer. This then returns the output\footnote{The Keras classes don't execute statements immediately, but construct a computation graph for later execution.} of the layer. The outputs can now be feed into another layer. After the model has been described in this manner, Keras can receive optimization parameters e.g. the loss function and compile the executable computation graph.\todo{is this section nessesary, or at the right place?}}

The model created corresponds to the model described in section \ref{BackgroundGQN}, with the exception that no latent variables $z$ are used. As encoder and decoder dense networks are used.

\dl{cimg('image series of how the network evolved, show output of network')}


\subsection{Saving and loading of network}
To make it easier to identify and reuse models a simple versioning system has been developed. If no specific model is specified to be used a new one of the specified architecture is created. The model is given a unique name consistent of the training date, a randomly generated human personal name for quick identification, a random numerical id and a version number.

If request is received to load a specific model, an automatic search is performed for the newest model of that id, which is then loaded instead. The versioning of a model is set up properly based on the version of the model that is currently trained.

Keras is used to automatically save the model during training in intervals, preventing total data loss on a system failure.


\subsection{Data Preprocessing}
First the data is loaded from from disk. Then the data points are normalized. This means that we make all inputs of the data have a range between zero and one. This is required for the used architecture to enable efficient training. The next step is to create a pairings of input data and output labels. This is simply done by choosing all the inputs and outputs from the same environment. These pairings constitute the training data for the model. The output of the model is just a vector, so that we need to rearrange it into a tensor of the original image dimensions. Then we can encode the tensor into a JPEG image.


\section[IPC]{Inter process communication}
Because Python runs outside the unity environment we need a way to send the network output to Unity. This is done by using a local UDP socket to send the JPEG images to a listening UDP socket in Unity. Because UDP is a connectionless protocol and doest give any guaranties that the send data will be received it is a very fast way to send the data. The chance of data getting lost on a local server is very slim and because we are sending a continuous stream we don't care if an image is lost, because we will receive a new one in a few milliseconds.


\section{Rendering}
Now the data received by the Unity UDP socket gets loaded into a texture.
In Unity the stream is decoded and rendered using custom Cg shaders. These shaders merges the network output stream with objects in the environment that are tagged to be visible in the combined render.
\dl{cimg('image with only network output')}
\dl{cimg('image unity + network output')}

Done by giving environment specific color, that is set to transparent, when unity render is layered over the network output.
\dl{cimg('img of unity environment where you see eveything has keyout color')}
Cull objects if hidden from camera pos in unity environment
